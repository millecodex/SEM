{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--------------------------------------------------------------\n",
    "# #-- S E A R C H   C R I T I C A L I T Y   S P R E A D S H E E T\n",
    "# #--------------------------------------------------------------\n",
    "# # using the csv file of CRITICALITY_SCORE and searching for \n",
    "# # repos from 200_repos; this excludes new/current projects\n",
    "# # download: https://storage.cloud.google.com/ossf-criticality-score\n",
    "# # UDPATE: looks like a monthly run is published here (wish I had of known!)\n",
    "# #--------------------------------------------------------------\n",
    "# # Read '200_repos.csv' into DataFrame df\n",
    "# #\n",
    "# # NaN is assigned to empty cells\n",
    "# df = pd.read_csv('200_repos.csv')\n",
    "# dfs = df[['CMC_id', 'source_code', 'forge']].copy()\n",
    "# dfc = pd.read_csv('project_criticality_all.csv')\n",
    "# num = 0\n",
    "# for row in dfs.itertuples():\n",
    "#     # only search if github\n",
    "#     if row.forge == 'github':\n",
    "#         # only search for strings; floats (NaN) are skipped\n",
    "#         if isinstance(row.source_code, str):\n",
    "#             url = str(row.source_code)\n",
    "#             # loop through df2 (criticality) looking for source code url\n",
    "#             for row2 in dfc.itertuples():\n",
    "#                 if url == row2.url:\n",
    "#                     dfs.at[row.Index, 'criticality'] = row2.criticality_score\n",
    "#                     num += 1\n",
    "#                     break\n",
    "#             sys.stdout.write(\".\")\n",
    "#             sys.stdout.flush()\n",
    "# print(str(num), 'criticality scores found and updated')\n",
    "\n",
    "# # update MERGED sheet with new data\n",
    "# # 'CMC_id' is the key, drop 'repo', and 'forge' before the merge\n",
    "# # to prevent duplicate columns\n",
    "# dfs.drop(columns = ['source_code', 'forge'], inplace = True)\n",
    "# dfm = pd.merge(df,dfs,on = ['CMC_id'], how = 'outer')\n",
    "\n",
    "# # write out new data\n",
    "# dfs.to_csv('200_crit.csv', encoding='utf-8', index = 0)\n",
    "# dfm.to_csv('200_merged.csv', encoding='utf-8', index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# -- C A L L   C R I T I C A L I T Y _ S C O R E--\n",
    "# require github token and command line access----\n",
    "#-------------------------------------------------\n",
    "#\n",
    "# Input:\n",
    "# 'cmc_repos_forge.csv' from prepare_repos.ipynb\n",
    "#\n",
    "# Outputs:\n",
    "# 'crit_output_all.csv' contains all the info; see sample output below\n",
    "# 'crit_only.csv' contains two columns: 'url:','criticality_score:'\n",
    "#\n",
    "# >> repo: https://github.com/ossf/criticality_score\n",
    "# 0. make sure github access token is exported to PATH (see methodology notes)\n",
    "# 1. install: pip3 install criticality-score\n",
    "# 2. check PATH: WARNING: The script criticality_score is installed in '/home/user/.local/bin' which is not on PATH.\n",
    "# >> export PATH=\"/home/user/.local/bin:$PATH\"\n",
    "# 3. get 'GITHUB_AUTH_TOKEN' and export path on command line or set env variable in jupyter\n",
    "# \n",
    "# Set the environment variable 'GITHUB_AUTH_TOKEN' from Jupyter\n",
    "# (this is a short-cut; in the future look into pycrosskit)\n",
    "# >key = 'GITHUB_AUTH_TOKEN'\n",
    "# >os.environ[key] = 'secret'\n",
    "#\n",
    "# read out the value\n",
    "# >value = os.getenv(key)\n",
    "# >print(\"Value of 'GITHUB_AUTH_TOKEN' environment variable :\", value) \n",
    "#\n",
    "# 4. run: criticality_score --repo https://github.com/bitcoin/bitcoin\n",
    "# >sample output:\n",
    "# '''     \n",
    "# ['name: bitcoin',\n",
    "#  'url: https://github.com/bitcoin/bitcoin',\n",
    "#  'language: C++',\n",
    "#  'created_since: 142',\n",
    "#  'updated_since: 0',\n",
    "#  'contributor_count: 961',\n",
    "#  'org_count: 4',\n",
    "#  'commit_frequency: 54.8',\n",
    "#  'recent_releases_count: 3',\n",
    "#  'updated_issues_count: 1920',\n",
    "#  'closed_issues_count: 1467',\n",
    "#  'comment_frequency: 2.7',\n",
    "#  'dependents_count: 348588',\n",
    "#  'criticality_score: 0.86651']\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'GITHUB_AUTH_TOKEN'\n",
    "os.environ[key] = 'xxx'\n",
    "\n",
    "# have a look\n",
    "value = os.getenv(key)\n",
    "print(\"Value of 'GITHUB_AUTH_TOKEN' environment variable :\", value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read '200_repos.csv' into DataFrame df\n",
    "#df = pd.read_csv('cmc_repos_forge.csv')\n",
    "df = pd.read_csv('merged_Jan_2023.csv')\n",
    "# keep 'source_code' location and 'forge'\n",
    "df_in = df[['source_code', 'forge', 'language:']].copy()\n",
    "df_in = df_in.rename(columns={'language:': 'language'})\n",
    "# subset dataframes for testing\n",
    "df_test1 = df_in.iloc[99:104].copy()\n",
    "df_test2 = df_in.iloc[:20].copy()\n",
    "df_test3 = df_in.iloc[99:104].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df200 = df_in.iloc[:199].copy()\n",
    "df400 = df_in.iloc[200:399].copy()\n",
    "df600 = df_in.iloc[400:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# dfParse builds a dataFrame using bash output\n",
    "#  @output the command line output from calling criticality_score\n",
    "#  @firstTime boolean to initialize the dataframe the first loop call\n",
    "#  @dataframe the dataframe to be updated and returned\n",
    "# ------------------------------------------------\n",
    "def dfParse(output, firstUpdate, dataframe):\n",
    "    jout = json.dumps(output)    #jout is a str\n",
    "    out_dict = json.loads(jout)   #out_dict is a list\n",
    "    \n",
    "    # catch a possible traceback\n",
    "    if 'Traceback' in out_dict[0]:\n",
    "        print('found traceback')\n",
    "        return dataframe\n",
    "    \n",
    "    # prepare the dataFrame, initialize with column headers the same\n",
    "    # as the criticality_score output\n",
    "    df = pd.DataFrame(out_dict)\n",
    "    df.rename(columns = {0:'metric'}, inplace = True)\n",
    "    df[['metric','value']] = df.metric.str.split(expand = True)\n",
    "    df = df.transpose(copy = True)\n",
    "\n",
    "    # remove index column (with labels 'metric' & 'value')\n",
    "    # and reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # rename columns according to first row; then drop the row\n",
    "    df = df.rename(columns = df.iloc[0]).drop(df.index[0])\n",
    "\n",
    "    if firstUpdate:\n",
    "        dataframe = df.copy()\n",
    "    else:\n",
    "        # append row[1] to df\n",
    "        dataframe = dataframe.append(df, ignore_index = True)   \n",
    "        \n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# main loop requires dataFrame: 'df_in'\n",
    "#                      returns: 'df_out'\n",
    "# df_out does not have CMC_id and some will be missed;\n",
    "# should be able to merge back on 'url:'=='source_code'\n",
    "# ------------------------------------------------------\n",
    "# This takes a while, criticality_score has a built-in\n",
    "# rate limiter for handling github API limits. It still \n",
    "# gets stuck over about 200 url requests.\n",
    "# for example:\n",
    "# >Rate limit exceeded, sleeping till reset: 334 seconds.\n",
    "# ------------------------------------------------------\n",
    "# Sample Output:\n",
    "#  223 total projects evaluated\n",
    "#  153 criticality scores updated\n",
    "#  70 repos private or missing\n",
    "#  Total time elapsed: 22.6 minutes\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Need to chunk this better to avoid the rate limiting and\n",
    "# its confusing with 200/400/600, etc.\n",
    "#df_in = df200.copy()\n",
    "#df_in = df400.copy()\n",
    "df_in = df600.copy()\n",
    "# df_in = df_test3.copy()\n",
    "\n",
    "start = time.time()\n",
    "total = 0\n",
    "updated = 0\n",
    "firstUpdate = True\n",
    "df_out = pd.DataFrame\n",
    "\n",
    "for row in df_in.itertuples():\n",
    "    # proceed if github and source_code is a string and not private\n",
    "    # added check 'and (pd.isna(row.language))' to update criticality columns Jan.2023, remove for future use\n",
    "    if (row.forge == 'github') and isinstance(row.source_code, str) and (row.source_code != 'private') and (pd.isna(row.language)):\n",
    "        cmd = 'criticality_score --repo ' + row.source_code\n",
    "        print(row.source_code)\n",
    "        output = !{cmd}\n",
    "        # if first element is ['name':'bitcoin'], output is as expected, can parse\n",
    "        if 'name' in output[0]: \n",
    "            df_out = dfParse(output, firstUpdate, df_out)\n",
    "            firstUpdate = False\n",
    "            updated += 1\n",
    "    total += 1\n",
    "    sys.stdout.write(\".\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "# log some output with a timer\n",
    "print('\\n',str(df_in.shape[0]), 'total projects evaluated\\n', \n",
    "      str(updated), 'criticality scores updated\\n', \n",
    "      str(total - updated), 'repos private or missing\\n', \n",
    "      'Total time elapsed:', round((time.time() - start)/60, 1), 'minutes')\n",
    "\n",
    "#df200 = df_out.copy()\n",
    "#df400 = df_out.copy()\n",
    "df600 = df_out.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out new data\n",
    "df200.to_csv('crit_output_200_Jan2023.csv', encoding='utf-8', index = 0)\n",
    "df400.to_csv('crit_output_400_Jan2023.csv', encoding='utf-8', index = 0)\n",
    "df600.to_csv('crit_output_600_Jan2023.csv', encoding='utf-8', index = 0)\n",
    "# dft = df_out[['url:','criticality_score:']].copy()\n",
    "# dft.to_csv('crit_only.csv', encoding='utf-8', index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in chunked data\n",
    "# df200 = pd.read_csv('crit_output_all_0-200.csv')\n",
    "# df400 = pd.read_csv('crit_output_all_200-400.csv')\n",
    "# df600 = pd.read_csv('crit_output_all_400-600.csv')\n",
    "# dfcrit200 = pd.read_csv('crit_only_0-200.csv')\n",
    "# dfcrit400 = pd.read_csv('crit_only_200-400.csv')\n",
    "# dfcrit600 = pd.read_csv('crit_only_400-600.csv')\n",
    "df200 = pd.read_csv('crit_output_all_200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data frames\n",
    "dfall = pd.concat([df200, df400, df600], ignore_index=True)\n",
    "# dfcrit = pd.concat([dfcrit200, dfcrit400, dfcrit600], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out merged frames\n",
    "dfall.to_csv('crit_output_Jan2023.csv', encoding='utf-8', index = 0)\n",
    "# dfcrit.to_csv('crit_only.csv', encoding='utf-8', index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge criticality with cmc_repos_forge\n",
    "#cmc_repos_forge = pd.read_csv(r\"/home/japple/localDev/health/cmc/cmc_repos_forge.csv\")\n",
    "\n",
    "original = pd.read_csv('merged_Jan_2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [crit_output_all] 'url:' == [cmc_repos_forge] 'source_code'\n",
    "dfall.drop(columns = ['name:'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename [crit_output_all] 'url:' to 'source_code' to match\n",
    "dfall.rename(columns = {'url:':'source_code'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = pd.merge(original, dfall, on = ['source_code'], how = 'outer')\n",
    "#dfm.sort_values(by='CMC_rank', ascending=False)\n",
    "#dfm.to_csv('merged_Jan2023.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to update criticality gaps\n",
    "original['source_code'] = original['source_code'].str.lower()\n",
    "dfall['source_code'] = dfall['source_code'].str.lower()\n",
    "for i, row in original.iterrows():\n",
    "    if row['source_code'] in dfall['source_code'].values:\n",
    "        dfall_row = dfall.loc[dfall['source_code'] == row['source_code']]\n",
    "        original.loc[i, 'language:'] = dfall_row['language:'].values[0]\n",
    "        original.loc[i, 'created_since:'] = dfall_row['created_since:'].values[0]\n",
    "        original.loc[i, 'updated_since:'] = dfall_row['updated_since:'].values[0]\n",
    "        original.loc[i, 'contributor_count:'] = dfall_row['contributor_count:'].values[0]\n",
    "        original.loc[i, 'org_count:'] = dfall_row['org_count:'].values[0]\n",
    "        original.loc[i, 'commit_frequency:'] = dfall_row['commit_frequency:'].values[0]\n",
    "        original.loc[i, 'recent_releases_count:'] = dfall_row['recent_releases_count:'].values[0]\n",
    "        original.loc[i, 'updated_issues_count:'] = dfall_row['updated_issues_count:'].values[0]\n",
    "        original.loc[i, 'closed_issues_count:'] = dfall_row['closed_issues_count:'].values[0]\n",
    "        original.loc[i, 'comment_frequency:'] = dfall_row['comment_frequency:'].values[0]\n",
    "        original.loc[i, 'dependents_count:'] = dfall_row['dependents_count:'].values[0]\n",
    "        original.loc[i, 'criticality_score:'] = dfall_row['criticality_score:'].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "original.to_csv('crit_updated_Jan2023.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge alexa with merged_checked_manually\n",
    "dfalexa = pd.read_csv(r\"/home/japple/localDev/health/webRank/alexa.csv\")\n",
    "dfsource = pd.read_csv(\"merged_checked_manually.csv\")\n",
    "# merge on CMC_id\n",
    "dft=dfalexa[['CMC_id', 'alexa_rank']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm=pd.merge(dfsource,dft,on=['CMC_id'], how='outer')\n",
    "dfm.sort_values(by='CMC_rank', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate CMC_ids were found\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# --C H E C K   F O R   D U P L I C A T E S-------------------\n",
    "# CMC_id column *may* contain duplicate entries (CAKE, FUN!?)\n",
    "# ------------------------------------------------------------\n",
    "if dfm[dfm.duplicated(['CMC_id'], keep='last')].empty:\n",
    "    print('No duplicate CMC_ids were found')\n",
    "else: \n",
    "    num = len(dfm[dfm.duplicated(['CMC_id'], keep='last')])\n",
    "    print(num, 'duplicates were found and deleted based on CMC_id:')\n",
    "    print(dfm[dfm.duplicated(['CMC_id'], keep='last')])\n",
    "    \n",
    "    # delete duplicates keeping 2nd entry\n",
    "    dfm.drop_duplicates(subset=['CMC_id'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.to_csv('merged.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # store a dataframe in the kernel\n",
    "# %store dfm\n",
    "\n",
    "# # retrieve a datafram\n",
    "# %store -r dfm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:43:08) [MSC v.1926 32 bit (Intel)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e631ceeaf344b75b560bc355a5cd0eb815e06623d4e569cc67edd0347f090c42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
